name: "LeNet"

layer {
	name: "mnist"
	type: "Python"
	python_param {
		module: "layers"
		layer: "MnistDataLayer"
	}
	include { phase: TRAIN}
	top: "data"
	top: "label"
}

layer {
	name: "mnist"
	type: "Input"
	top: "data"
	include { phase: TEST}
	input_param {
		# These dimensions are purely for sake of example;
		# see eval.py for how to reshape the net to the given input size.
		shape { dim: 1 dim: 1 dim: 28 dim: 28 }
	}
}

layer {
	name: "conv1"
	type: "Convolution"
	param { lr_mult: 1 }
	param { lr_mult: 2 }
	convolution_param {
		num_output: 20
		kernel_size: 5
		stride: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
	bottom: "data"
	top: "conv1"
}

layer {
	name: "pool1"
	type: "Pooling"
	pooling_param {
		kernel_size: 2
		stride: 2
		pool: MAX
	}
	bottom: "conv1"
	top: "pool1"
}

layer {
	name: "conv2"
	type: "Convolution"
	bottom: "pool1"
	top: "conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 50
		kernel_size: 5
		stride: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
		}
	}
}

layer {
	name: "pool2"
	type: "Pooling"
	bottom: "conv2"
	top: "pool2"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

layer {
	name: "ip1"
	type: "InnerProduct"
	param { lr_mult: 1}
	param { lr_mult: 2}
	inner_product_param {
		num_output: 500
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
		}
	}
	bottom: "pool2"
	top: "ip1"
}

layer {
	name: "relu1"
	type: "ReLU"
	bottom: "ip1"
	top: "ip1"
}

layer {
	name: "ip2"
	type: "InnerProduct"
	param { lr_mult: 1}
	param { lr_mult: 2}
	inner_product_param {
		num_output: 10
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
	bottom: "ip1"
	top: "ip2"
}

layer {
	name: "loss"
	type: "SoftmaxWithLoss"
	bottom: "ip2"
	bottom: "label"
	include { phase: TRAIN}
}
